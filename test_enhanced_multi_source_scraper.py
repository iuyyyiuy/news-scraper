#!/usr/bin/env python3
"""
Test Enhanced Multi-Source Scraper with Database Duplicate Detection
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from datetime import date, timedelta
from scraper.core.multi_source_scraper import MultiSourceScraper
from scraper.core.storage import InMemoryDataStore
from scraper.core import Config

def test_enhanced_multi_source_scraper():
    """Test the enhanced multi-source scraper with database duplicate detection"""
    
    print("ğŸ§ª Testing Enhanced Multi-Source Scraper")
    print("=" * 60)
    
    # Create config
    config = Config(
        target_url="https://www.theblockbeats.info/newsflash",
        max_articles=10,  # Small number for testing
        request_delay=1.0,
        timeout=30,
        max_retries=2
    )
    
    # Create in-memory data store
    data_store = InMemoryDataStore()
    
    # Date range: last 3 days
    end_date = date.today()
    start_date = end_date - timedelta(days=3)
    
    # Security keywords
    keywords = [
        "å®‰å…¨é—®é¢˜", "é»‘å®¢", "è¢«ç›—", "æ¼æ´", "æ”»å‡»", "æ¶æ„è½¯ä»¶", "ç›—çªƒ",
        "CoinEx", "ViaBTC", "ç ´äº§", "æ‰§æ³•", "ç›‘ç®¡", "æ´—é’±", "KYC",
        "åˆè§„", "ç‰Œç…§", "é£æ§", "è¯ˆéª—", "çªå‘", "rug pull", "ä¸‹æ¶"
    ]
    
    # Progress callback
    def progress_callback(source: str, articles_found: int, articles_scraped: int):
        print(f"ğŸ“Š {source.upper()}: æ‰¾åˆ° {articles_found} ç¯‡ï¼ŒæŠ“å– {articles_scraped} ç¯‡")
    
    # Log callback
    def log_callback(message: str, log_type: str = 'info', source: str = None, show_in_all: bool = True):
        if log_type == 'error':
            print(f"âŒ {message}")
        elif log_type == 'success':
            print(f"âœ… {message}")
        else:
            print(f"â„¹ï¸  {message}")
    
    try:
        print(f"ğŸ“… æ—¥æœŸèŒƒå›´: {start_date} åˆ° {end_date}")
        print(f"ğŸ”‘ å…³é”®è¯: {len(keywords)} ä¸ªå®‰å…¨ç›¸å…³å…³é”®è¯")
        print(f"ğŸ“° æ¥æº: BlockBeats (ä»…æµ‹è¯•)")
        print()
        
        # Create scraper with enhanced duplicate detection
        scraper = MultiSourceScraper(
            config=config,
            data_store=data_store,
            start_date=start_date,
            end_date=end_date,
            keywords_filter=keywords,
            sources=['blockbeats'],  # Only BlockBeats for testing
            enable_deduplication=True,  # Enable enhanced duplicate detection
            progress_callback=progress_callback,
            log_callback=log_callback
        )
        
        print("ğŸš€ å¼€å§‹æµ‹è¯•å¢å¼ºå»é‡åŠŸèƒ½...")
        print()
        
        # Run scraper
        result = scraper.scrape(parallel=False)
        
        # Get articles
        articles = data_store.get_all_articles()
        
        print()
        print("ğŸ“Š æµ‹è¯•ç»“æœ:")
        print(f"   - æ€»è®¡æ£€æŸ¥æ–‡ç« : {result.total_articles_found}")
        print(f"   - æœ€ç»ˆä¿å­˜æ–‡ç« : {result.articles_scraped}")
        print(f"   - å¤±è´¥æ–‡ç« : {result.articles_failed}")
        print(f"   - å¤„ç†è€—æ—¶: {result.duration_seconds:.2f} ç§’")
        
        if result.errors:
            print(f"   - é”™è¯¯æ•°é‡: {len(result.errors)}")
            for error in result.errors[:3]:  # Show first 3 errors
                print(f"     * {error}")
        
        print()
        print("ğŸ“° æŠ“å–åˆ°çš„æ–‡ç« :")
        for i, article in enumerate(articles[:5], 1):  # Show first 5 articles
            print(f"   [{i}] {article.title[:60]}...")
            print(f"       æ¥æº: {getattr(article, 'source_website', 'Unknown')}")
            print(f"       URL: {getattr(article, 'url', 'No URL')}")
            print()
        
        if len(articles) > 5:
            print(f"   ... è¿˜æœ‰ {len(articles) - 5} ç¯‡æ–‡ç« ")
        
        print("âœ… å¢å¼ºå¤šæºæŠ“å–æµ‹è¯•å®Œæˆ")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_enhanced_multi_source_scraper()
    sys.exit(0 if success else 1)