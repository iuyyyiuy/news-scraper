{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“° News Scraper for BlockBeats\n",
    "\n",
    "This notebook scrapes news articles from theblockbeats.info with keyword and date filtering.\n",
    "\n",
    "## Features:\n",
    "- âœ… Scrape flash news articles\n",
    "- âœ… Filter by keywords (Chinese & English)\n",
    "- âœ… Filter by date range (last N days)\n",
    "- âœ… Export to CSV\n",
    "- âœ… Automatic date extraction from article text\n",
    "\n",
    "## How to use:\n",
    "1. Run all cells in order\n",
    "2. Configure your settings in the \"Configuration\" section\n",
    "3. Run the scraper\n",
    "4. Download the CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests beautifulsoup4 lxml python-dateutil -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Step 2: Import Libraries and Define Core Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil import parser as date_parser\n",
    "from typing import List, Optional, Dict\n",
    "from dataclasses import dataclass\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step 3: Define Data Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Article:\n",
    "    \"\"\"Represents a scraped news article.\"\"\"\n",
    "    url: str\n",
    "    title: str\n",
    "    publication_date: Optional[datetime]\n",
    "    author: Optional[str]\n",
    "    body_text: str\n",
    "    scraped_at: datetime\n",
    "    source_website: str\n",
    "    matched_keywords: Optional[List[str]] = None\n",
    "\n",
    "print(\"âœ… Data models defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŒ Step 4: HTTP Client Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_url(url: str, timeout: int = 30, max_retries: int = 3) -> str:\n",
    "    \"\"\"Fetch URL with retry logic.\"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (compatible; NewsScraperBot/1.0)',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "    }\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=timeout)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "print(\"âœ… HTTP client functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” Step 5: Parser Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_urls(html: str, base_url: str) -> List[str]:\n",
    "    \"\"\"Extract article URLs from listing page.\"\"\"\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    article_urls = []\n",
    "    \n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link.get('href')\n",
    "        \n",
    "        # Only accept /flash/{number} URLs\n",
    "        if re.match(r'^/flash/\\d+$', href):\n",
    "            absolute_url = urljoin(base_url, href)\n",
    "            if absolute_url not in article_urls:\n",
    "                article_urls.append(absolute_url)\n",
    "    \n",
    "    return article_urls\n",
    "\n",
    "\n",
    "def extract_date_from_body(body_text: str) -> Optional[datetime]:\n",
    "    \"\"\"Extract date from body text patterns.\"\"\"\n",
    "    # Pattern: \"BlockBeats æ¶ˆæ¯ï¼Œ11 æœˆ 11 æ—¥ï¼Œ\"\n",
    "    pattern = r'BlockBeats\\s*æ¶ˆæ¯\\s*ï¼Œ\\s*(\\d{1,2})\\s*æœˆ\\s*(\\d{1,2})\\s*æ—¥'\n",
    "    match = re.search(pattern, body_text)\n",
    "    if match:\n",
    "        month = int(match.group(1))\n",
    "        day = int(match.group(2))\n",
    "        year = datetime.now().year\n",
    "        try:\n",
    "            return datetime(year, month, day)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_article(html: str, url: str) -> Article:\n",
    "    \"\"\"Parse article from HTML.\"\"\"\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "    # Extract title\n",
    "    title_tag = soup.find('h1')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"No Title\"\n",
    "    \n",
    "    # Extract author\n",
    "    author = \"@BlockBeats\"  # Default for this site\n",
    "    \n",
    "    # Extract body text\n",
    "    body_element = soup.select_one('.flash-top, .flash-top-border')\n",
    "    if body_element:\n",
    "        # Remove unwanted elements\n",
    "        for unwanted in body_element.select('script, style, nav, header, footer'):\n",
    "            unwanted.decompose()\n",
    "        body_text = body_element.get_text(separator=' ', strip=True)\n",
    "    else:\n",
    "        body_text = \"\"\n",
    "    \n",
    "    if not body_text or len(body_text) < 50:\n",
    "        raise ValueError(\"Could not extract article body\")\n",
    "    \n",
    "    # Extract content starting from 'BlockBeats æ¶ˆæ¯' and remove footer\n",
    "    # Use simple string find instead of regex for better reliability\n",
    "    blockbeats_marker = 'BlockBeats æ¶ˆæ¯'\n",
    "    blockbeats_pos = body_text.find(blockbeats_marker)\n",
    "    \n",
    "    if blockbeats_pos != -1:\n",
    "        # Start from BlockBeats æ¶ˆæ¯\n",
    "        body_text = body_text[blockbeats_pos:]\n",
    "        \n",
    "        # Remove footer content - find the earliest footer marker\n",
    "        footer_markers = [\n",
    "            'AI è§£è¯»',\n",
    "            'å±•å¼€',\n",
    "            'åŽŸæ–‡é“¾æŽ¥',\n",
    "            'ä¸¾æŠ¥',\n",
    "            'çº é”™/ä¸¾æŠ¥',\n",
    "            'æœ¬å¹³å°çŽ°å·²å…¨é¢é›†æˆ',\n",
    "            'çƒ­é—¨æ–‡ç« '\n",
    "        ]\n",
    "        \n",
    "        earliest_pos = len(body_text)\n",
    "        for marker in footer_markers:\n",
    "            pos = body_text.find(marker)\n",
    "            if pos != -1 and pos < earliest_pos:\n",
    "                earliest_pos = pos\n",
    "        \n",
    "        # Cut at the earliest footer marker\n",
    "        if earliest_pos < len(body_text):\n",
    "            body_text = body_text[:earliest_pos]\n",
    "    \n",
    "    body_text = body_text.strip()\n",
    "    \n",
    "    # Extract date from body text\n",
    "    publication_date = extract_date_from_body(body_text)\n",
    "    \n",
    "    return Article(\n",
    "        url=url,\n",
    "        title=title,\n",
    "        publication_date=publication_date,\n",
    "        author=author,\n",
    "        body_text=body_text,\n",
    "        scraped_at=datetime.now(),\n",
    "        source_website=\"www.theblockbeats.info\"\n",
    "    )\n",
    "\n",
    "print(\"âœ… Parser functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Step 6: Filtering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_save_article(article: Article, days_filter: Optional[int], keywords_filter: Optional[List[str]]) -> bool:\n",
    "    \"\"\"Check if article passes filters.\"\"\"\n",
    "    # Date filter\n",
    "    if days_filter is not None and article.publication_date:\n",
    "        cutoff_date = datetime.now() - timedelta(days=days_filter)\n",
    "        if article.publication_date < cutoff_date:\n",
    "            return False\n",
    "    \n",
    "    # Keyword filter\n",
    "    if keywords_filter:\n",
    "        article_text = f\"{article.title} {article.body_text}\".lower()\n",
    "        matched = [kw for kw in keywords_filter if kw.lower() in article_text]\n",
    "        \n",
    "        if not matched:\n",
    "            return False\n",
    "        \n",
    "        article.matched_keywords = matched\n",
    "    \n",
    "    return True\n",
    "\n",
    "print(\"âœ… Filtering functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Step 7: Save to CSV Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(articles: List[Article], filename: str):\n",
    "    \"\"\"Save articles to CSV file.\"\"\"\n",
    "    fieldnames = ['publication_date', 'title', 'body_text', 'url', 'matched_keywords']\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        for article in articles:\n",
    "            row = {\n",
    "                'publication_date': article.publication_date.strftime('%Y-%m-%d') if article.publication_date else '',\n",
    "                'title': article.title,\n",
    "                'body_text': article.body_text,\n",
    "                'url': article.url,\n",
    "                'matched_keywords': ', '.join(article.matched_keywords) if article.matched_keywords else ''\n",
    "            }\n",
    "            writer.writerow(row)\n",
    "\n",
    "print(\"âœ… CSV save function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Step 8: Main Scraper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_news(target_url: str, max_articles: int, keywords: List[str], days: Optional[int], output_file: str, request_delay: float = 2.0):\n",
    "    \"\"\"Main scraping function.\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"ðŸ“° NEWS SCRAPER\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Target URL:      {target_url}\")\n",
    "    print(f\"Max Articles:    {max_articles}\")\n",
    "    print(f\"Keywords:        {', '.join(keywords)}\")\n",
    "    if days:\n",
    "        print(f\"Date Filter:     Last {days} days\")\n",
    "    print(f\"Output File:     {output_file}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    articles_scraped = []\n",
    "    articles_failed = 0\n",
    "    errors = []\n",
    "    \n",
    "    try:\n",
    "        # Fetch listing page\n",
    "        print(\"ðŸ“¡ Fetching article listing page...\")\n",
    "        html = fetch_url(target_url)\n",
    "        \n",
    "        # Extract article URLs\n",
    "        print(\"ðŸ” Extracting article URLs...\")\n",
    "        article_urls = extract_article_urls(html, target_url)\n",
    "        print(f\"âœ… Found {len(article_urls)} article URLs\")\n",
    "        \n",
    "        # Limit to max_articles\n",
    "        article_urls = article_urls[:max_articles]\n",
    "        print(f\"ðŸ“ Processing {len(article_urls)} articles\\n\")\n",
    "        \n",
    "        # Process each article\n",
    "        for index, article_url in enumerate(article_urls, 1):\n",
    "            try:\n",
    "                print(f\"[{index}/{len(article_urls)}] Processing: {article_url}\")\n",
    "                \n",
    "                # Fetch article\n",
    "                article_html = fetch_url(article_url)\n",
    "                \n",
    "                # Parse article\n",
    "                article = parse_article(article_html, article_url)\n",
    "                \n",
    "                # Check filters\n",
    "                if should_save_article(article, days, keywords):\n",
    "                    articles_scraped.append(article)\n",
    "                    print(f\"   âœ… Scraped: {article.title[:60]}...\")\n",
    "                else:\n",
    "                    print(f\"   â­ï¸  Filtered out\")\n",
    "                \n",
    "                # Rate limiting\n",
    "                if index < len(article_urls):\n",
    "                    time.sleep(request_delay)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                articles_failed += 1\n",
    "                error_msg = f\"Failed to scrape {article_url}: {str(e)}\"\n",
    "                errors.append(error_msg)\n",
    "                print(f\"   âŒ Error: {str(e)}\")\n",
    "        \n",
    "        # Save to CSV\n",
    "        if articles_scraped:\n",
    "            print(f\"\\nðŸ’¾ Saving {len(articles_scraped)} articles to {output_file}...\")\n",
    "            save_to_csv(articles_scraped, output_file)\n",
    "        \n",
    "        # Summary\n",
    "        duration = time.time() - start_time\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"ðŸ“Š SCRAPING SUMMARY\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Articles Found:    {len(article_urls)}\")\n",
    "        print(f\"Articles Scraped:  {len(articles_scraped)}\")\n",
    "        print(f\"Articles Failed:   {articles_failed}\")\n",
    "        print(f\"Duration:          {duration:.2f} seconds\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        if articles_scraped:\n",
    "            print(f\"âœ… Successfully scraped {len(articles_scraped)} article(s)!\")\n",
    "            print(f\"ðŸ“„ Results saved to: {output_file}\")\n",
    "        else:\n",
    "            print(\"âš ï¸  No articles matched your filters\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Fatal error: {e}\")\n",
    "        raise\n",
    "\n",
    "print(\"âœ… Main scraper function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ Step 9: Configuration\n",
    "\n",
    "**Customize these settings:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CONFIGURATION ==========\n",
    "\n",
    "# Target website\n",
    "TARGET_URL = \"https://www.theblockbeats.info/newsflash\"\n",
    "\n",
    "# Maximum number of articles to scrape\n",
    "MAX_ARTICLES = 20\n",
    "\n",
    "# Keywords to filter (articles must contain at least one)\n",
    "# You can use Chinese or English keywords\n",
    "KEYWORDS = [\"BTC\", \"ETH\", \"Bitcoin\", \"Ethereum\", \"Uniswap\", \"é»‘å®¢\", \"ç›‘ç®¡\"]\n",
    "\n",
    "# Date filter: only scrape articles from last N days (set to None to disable)\n",
    "DAYS_FILTER = 7  # Last 7 days\n",
    "\n",
    "# Output filename with timestamp\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "OUTPUT_FILE = f\"crypto_news_{timestamp}.csv\"\n",
    "\n",
    "# Delay between requests (seconds) - be respectful to the server!\n",
    "REQUEST_DELAY = 2.0\n",
    "\n",
    "# ===================================\n",
    "\n",
    "print(\"âœ… Configuration set!\")\n",
    "print(f\"\\nYour settings:\")\n",
    "print(f\"  - Max articles: {MAX_ARTICLES}\")\n",
    "print(f\"  - Keywords: {', '.join(KEYWORDS)}\")\n",
    "print(f\"  - Date filter: Last {DAYS_FILTER} days\" if DAYS_FILTER else \"  - Date filter: Disabled\")\n",
    "print(f\"  - Output file: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¬ Step 10: Run the Scraper!\n",
    "\n",
    "**Click the play button to start scraping:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the scraper\n",
    "scrape_news(\n",
    "    target_url=TARGET_URL,\n",
    "    max_articles=MAX_ARTICLES,\n",
    "    keywords=KEYWORDS,\n",
    "    days=DAYS_FILTER,\n",
    "    output_file=OUTPUT_FILE,\n",
    "    request_delay=REQUEST_DELAY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¥ Step 11: Preview and Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the results\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(OUTPUT_FILE)\n",
    "    print(f\"\\nðŸ“Š Preview of {OUTPUT_FILE}:\")\n",
    "    print(f\"\\nTotal articles: {len(df)}\\n\")\n",
    "    print(df[['title', 'publication_date', 'matched_keywords']].head(10))\n",
    "    \n",
    "    print(f\"\\nðŸ’¡ To download the file:\")\n",
    "    print(f\"   1. Click the folder icon on the left sidebar\")\n",
    "    print(f\"   2. Find '{OUTPUT_FILE}'\")\n",
    "    print(f\"   3. Right-click and select 'Download'\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ No results file found. Please run the scraper first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ Notes\n",
    "\n",
    "- **Rate Limiting**: The scraper waits 2 seconds between requests by default. Don't set this too low!\n",
    "- **Keywords**: Articles must contain at least ONE of your keywords\n",
    "- **Date Format**: Dates are extracted from article text and formatted as YYYY-MM-DD\n",
    "- **URL Format**: Only scrapes URLs in format `/flash/{number}`\n",
    "\n",
    "## ðŸ”„ To Run Again\n",
    "\n",
    "1. Modify the configuration in Step 9\n",
    "2. Re-run Step 10\n",
    "3. Preview results in Step 11"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
