#!/usr/bin/env python3
"""
Final Test: Enhanced Duplicate Detection System
Test the complete system with real scraping to ensure duplicates are properly handled
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from datetime import date, timedelta
from scraper.core.multi_source_scraper import MultiSourceScraper
from scraper.core.storage import InMemoryDataStore
from scraper.core import Config

def test_final_enhanced_duplicate_system():
    """Final comprehensive test of the enhanced duplicate detection system"""
    
    print("ğŸ¯ Final Test: Enhanced Duplicate Detection System")
    print("=" * 70)
    
    # Create config for real scraping
    config = Config(
        target_url="https://www.theblockbeats.info/newsflash",
        max_articles=30,  # Moderate number to test duplicates
        request_delay=1.0,
        timeout=30,
        max_retries=2
    )
    
    # Create in-memory data store
    data_store = InMemoryDataStore()
    
    # Date range: last 5 days (more likely to have duplicates)
    end_date = date.today()
    start_date = end_date - timedelta(days=5)
    
    # Security keywords
    keywords = [
        "å®‰å…¨é—®é¢˜", "é»‘å®¢", "è¢«ç›—", "æ¼æ´", "æ”»å‡»", "æ¶æ„è½¯ä»¶", "ç›—çªƒ",
        "CoinEx", "ViaBTC", "ç ´äº§", "æ‰§æ³•", "ç›‘ç®¡", "æ´—é’±", "KYC",
        "åˆè§„", "ç‰Œç…§", "é£æ§", "è¯ˆéª—", "çªå‘", "rug pull", "ä¸‹æ¶"
    ]
    
    # Track duplicate detection results
    duplicate_stats = {
        'total_found': 0,
        'total_unique': 0,
        'duplicates_removed': 0,
        'methods_used': {}
    }
    
    # Progress callback
    def progress_callback(source: str, articles_found: int, articles_scraped: int):
        print(f"ğŸ“Š {source.upper()}: æ‰¾åˆ° {articles_found} ç¯‡ï¼Œå¤„ç† {articles_scraped} ç¯‡")
    
    # Log callback to track duplicate detection
    def log_callback(message: str, log_type: str = 'info', source: str = None, show_in_all: bool = True):
        if "é‡å¤æ–‡ç« " in message:
            # Extract duplicate count
            import re
            match = re.search(r'ç§»é™¤ (\d+) ç¯‡é‡å¤æ–‡ç« ', message)
            if match:
                duplicate_stats['duplicates_removed'] = int(match.group(1))
        
        if "URLåŒ¹é…" in message or "æ ‡é¢˜åŒ¹é…" in message or "å†…å®¹åŒ¹é…" in message or "ç›¸ä¼¼æ ‡é¢˜" in message:
            # Track duplicate detection methods
            import re
            for method in ["URLåŒ¹é…", "æ ‡é¢˜åŒ¹é…", "å†…å®¹åŒ¹é…", "ç›¸ä¼¼æ ‡é¢˜"]:
                if method in message:
                    match = re.search(rf'{method}: (\d+) ç¯‡', message)
                    if match:
                        duplicate_stats['methods_used'][method] = int(match.group(1))
        
        if log_type == 'error':
            print(f"âŒ {message}")
        elif log_type == 'success':
            print(f"âœ… {message}")
        else:
            print(f"â„¹ï¸  {message}")
    
    try:
        print(f"ğŸ“… æµ‹è¯•å‚æ•°:")
        print(f"   - æ—¥æœŸèŒƒå›´: {start_date} åˆ° {end_date} (5å¤©)")
        print(f"   - å…³é”®è¯: {len(keywords)} ä¸ªå®‰å…¨ç›¸å…³å…³é”®è¯")
        print(f"   - æœ€å¤§æ–‡ç« æ•°: {config.max_articles}")
        print(f"   - æ¥æº: BlockBeats")
        print(f"   - å¢å¼ºå»é‡: å¯ç”¨")
        print()
        
        # Create scraper with enhanced duplicate detection
        scraper = MultiSourceScraper(
            config=config,
            data_store=data_store,
            start_date=start_date,
            end_date=end_date,
            keywords_filter=keywords,
            sources=['blockbeats'],
            enable_deduplication=True,  # This enables enhanced duplicate detection
            progress_callback=progress_callback,
            log_callback=log_callback
        )
        
        print("ğŸš€ å¼€å§‹æœ€ç»ˆæµ‹è¯•...")
        print()
        
        # Get initial stats from enhanced duplicate detector
        initial_stats = scraper.enhanced_duplicate_detector.get_stats()
        print(f"ğŸ“Š æ•°æ®åº“ä¸­å·²æœ‰æ–‡ç« :")
        print(f"   - URLs: {initial_stats['urls_tracked']}")
        print(f"   - æ ‡é¢˜: {initial_stats['titles_tracked']}")
        print(f"   - å†…å®¹å“ˆå¸Œ: {initial_stats['content_hashes_tracked']}")
        print()
        
        # Run scraper
        result = scraper.scrape(parallel=False)
        
        # Get final articles
        articles = data_store.get_all_articles()
        duplicate_stats['total_found'] = result.total_articles_found
        duplicate_stats['total_unique'] = len(articles)
        
        print()
        print("ğŸ¯ æœ€ç»ˆæµ‹è¯•ç»“æœ:")
        print("=" * 50)
        print(f"âœ… æ€»è®¡æ£€æŸ¥æ–‡ç« : {result.total_articles_found}")
        print(f"âœ… æœ€ç»ˆå”¯ä¸€æ–‡ç« : {len(articles)}")
        print(f"âœ… é‡å¤æ–‡ç« ç§»é™¤: {duplicate_stats['duplicates_removed']}")
        print(f"âœ… å¤±è´¥æ–‡ç« : {result.articles_failed}")
        print(f"âœ… å¤„ç†è€—æ—¶: {result.duration_seconds:.2f} ç§’")
        
        if duplicate_stats['methods_used']:
            print()
            print("ğŸ” å»é‡æ–¹æ³•ç»Ÿè®¡:")
            for method, count in duplicate_stats['methods_used'].items():
                print(f"   - {method}: {count} ç¯‡")
        
        if result.errors:
            print(f"âš ï¸  é”™è¯¯æ•°é‡: {len(result.errors)}")
        
        print()
        print("ğŸ“° æŠ“å–åˆ°çš„å”¯ä¸€æ–‡ç« :")
        for i, article in enumerate(articles[:3], 1):  # Show first 3 articles
            print(f"   [{i}] {article.title[:60]}...")
            print(f"       æ¥æº: {getattr(article, 'source_website', 'Unknown')}")
            print(f"       æ—¥æœŸ: {getattr(article, 'publication_date', 'Unknown')}")
            print()
        
        if len(articles) > 3:
            print(f"   ... è¿˜æœ‰ {len(articles) - 3} ç¯‡å”¯ä¸€æ–‡ç« ")
        
        print()
        print("ğŸ‰ å¢å¼ºå»é‡ç³»ç»Ÿæµ‹è¯•ç»“æœ:")
        
        # Evaluate success
        success_criteria = [
            ("ç³»ç»Ÿæ­£å¸¸è¿è¡Œ", result.total_articles_found > 0),
            ("æˆåŠŸæ£€æµ‹é‡å¤", duplicate_stats['duplicates_removed'] >= 0),
            ("ä¿ç•™å”¯ä¸€æ–‡ç« ", len(articles) > 0),
            ("æ— ä¸¥é‡é”™è¯¯", len(result.errors) == 0)
        ]
        
        all_passed = True
        for criterion, passed in success_criteria:
            status = "âœ…" if passed else "âŒ"
            print(f"   {status} {criterion}")
            if not passed:
                all_passed = False
        
        if all_passed:
            print()
            print("ğŸŠ æ‰€æœ‰æµ‹è¯•é€šè¿‡! å¢å¼ºå»é‡ç³»ç»Ÿå·¥ä½œæ­£å¸¸!")
            print("   - æ–°é—»æŠ“å–åŠŸèƒ½ä¸­çš„é‡å¤æ–‡ç« é—®é¢˜å·²è§£å†³")
            print("   - ç³»ç»Ÿç°åœ¨ä¼šæ£€æŸ¥æ•°æ®åº“ä¸­çš„ç°æœ‰æ–‡ç« ")
            print("   - å¤šå±‚å»é‡æ£€æµ‹ç¡®ä¿ä¸ä¼šæ˜¾ç¤ºé‡å¤å†…å®¹")
        else:
            print()
            print("âš ï¸  éƒ¨åˆ†æµ‹è¯•æœªé€šè¿‡ï¼Œéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥")
        
        return all_passed
        
    except Exception as e:
        print(f"âŒ æœ€ç»ˆæµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_final_enhanced_duplicate_system()
    sys.exit(0 if success else 1)