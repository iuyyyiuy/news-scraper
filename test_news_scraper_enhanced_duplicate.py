#!/usr/bin/env python3
"""
Test News Scraper Function with Enhanced Duplicate Detection
"""

import requests
import json
import time
import sys

def test_news_scraper_enhanced_duplicate():
    """Test the news scraper function with enhanced duplicate detection"""
    
    print("ğŸ§ª Testing News Scraper Function - Enhanced Duplicate Detection")
    print("=" * 70)
    
    base_url = "http://localhost:5000"
    
    # Test parameters
    test_params = {
        "days_filter": 3,  # Last 3 days
        "keywords": [
            "å®‰å…¨é—®é¢˜", "é»‘å®¢", "è¢«ç›—", "æ¼æ´", "æ”»å‡»", "æ¶æ„è½¯ä»¶", "ç›—çªƒ",
            "CoinEx", "ViaBTC", "ç ´äº§", "æ‰§æ³•", "ç›‘ç®¡", "æ´—é’±", "KYC",
            "åˆè§„", "ç‰Œç…§", "é£æ§", "è¯ˆéª—", "çªå‘", "rug pull", "ä¸‹æ¶"
        ],
        "max_articles": 20,  # Small number for testing
        "sources": ["blockbeats"],  # Only BlockBeats
        "enable_deduplication": True  # Enable enhanced duplicate detection
    }
    
    try:
        print(f"ğŸ“… æµ‹è¯•å‚æ•°:")
        print(f"   - æ—¥æœŸèŒƒå›´: æœ€è¿‘ {test_params['days_filter']} å¤©")
        print(f"   - å…³é”®è¯: {len(test_params['keywords'])} ä¸ªå®‰å…¨ç›¸å…³å…³é”®è¯")
        print(f"   - æœ€å¤§æ–‡ç« æ•°: {test_params['max_articles']}")
        print(f"   - æ¥æº: {', '.join(test_params['sources'])}")
        print(f"   - å»é‡: {'å¯ç”¨' if test_params['enable_deduplication'] else 'ç¦ç”¨'}")
        print()
        
        # Start scraping session
        print("ğŸš€ å¯åŠ¨æ–°é—»æŠ“å–ä¼šè¯...")
        response = requests.post(f"{base_url}/api/scrape", json=test_params)
        
        if response.status_code != 200:
            print(f"âŒ å¯åŠ¨å¤±è´¥: {response.status_code} - {response.text}")
            return False
        
        session_data = response.json()
        session_id = session_data["session_id"]
        print(f"âœ… ä¼šè¯å·²å¯åŠ¨: {session_id}")
        print()
        
        # Monitor progress
        print("ğŸ“Š ç›‘æ§æŠ“å–è¿›åº¦...")
        last_log_count = 0
        duplicate_info = []
        
        while True:
            # Get session status
            status_response = requests.get(f"{base_url}/api/session/{session_id}")
            if status_response.status_code != 200:
                print(f"âŒ è·å–çŠ¶æ€å¤±è´¥: {status_response.status_code}")
                break
            
            session_info = status_response.json()
            status = session_info["status"]
            
            # Show new logs
            logs = session_info.get("logs", [])
            if len(logs) > last_log_count:
                for log in logs[last_log_count:]:
                    message = log["message"]
                    log_type = log["log_type"]
                    
                    # Track duplicate detection messages
                    if "é‡å¤æ–‡ç« " in message or "å»é‡" in message:
                        duplicate_info.append(message)
                    
                    if log_type == "error":
                        print(f"âŒ {message}")
                    elif log_type == "success":
                        print(f"âœ… {message}")
                    else:
                        print(f"â„¹ï¸  {message}")
                
                last_log_count = len(logs)
            
            # Check if completed
            if status in ["completed", "failed"]:
                break
            
            time.sleep(2)
        
        print()
        print("ğŸ“Š æœ€ç»ˆç»“æœ:")
        
        if status == "completed":
            result = session_info.get("result", {})
            articles = session_info.get("articles", [])
            
            print(f"âœ… æŠ“å–å®Œæˆ!")
            print(f"   - æ€»è®¡æ£€æŸ¥æ–‡ç« : {result.get('total_articles_found', 0)}")
            print(f"   - æœ€ç»ˆä¿å­˜æ–‡ç« : {result.get('articles_scraped', 0)}")
            print(f"   - å¤±è´¥æ–‡ç« : {result.get('articles_failed', 0)}")
            print(f"   - å¤„ç†è€—æ—¶: {result.get('duration_seconds', 0):.2f} ç§’")
            
            if result.get("errors"):
                print(f"   - é”™è¯¯æ•°é‡: {len(result['errors'])}")
            
            print()
            print("ğŸ” å»é‡ä¿¡æ¯:")
            if duplicate_info:
                for info in duplicate_info:
                    print(f"   - {info}")
            else:
                print("   - æœªå‘ç°å»é‡ç›¸å…³ä¿¡æ¯")
            
            print()
            print("ğŸ“° æŠ“å–åˆ°çš„æ–‡ç« :")
            for i, article in enumerate(articles[:5], 1):
                print(f"   [{i}] {article['title'][:60]}...")
                print(f"       æ¥æº: {article.get('source', 'Unknown')}")
                print(f"       æ—¥æœŸ: {article.get('publication_date', 'Unknown')}")
                print()
            
            if len(articles) > 5:
                print(f"   ... è¿˜æœ‰ {len(articles) - 5} ç¯‡æ–‡ç« ")
            
            print("âœ… æ–°é—»æŠ“å–åŠŸèƒ½æµ‹è¯•å®Œæˆ - å¢å¼ºå»é‡åŠŸèƒ½æ­£å¸¸å·¥ä½œ!")
            return True
            
        else:
            print(f"âŒ æŠ“å–å¤±è´¥: {status}")
            if session_info.get("error"):
                print(f"   é”™è¯¯: {session_info['error']}")
            return False
    
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_news_scraper_enhanced_duplicate()
    sys.exit(0 if success else 1)