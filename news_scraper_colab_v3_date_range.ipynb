{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“° News Scraper for BlockBeats - Date Range Version\n",
    "\n",
    "This notebook scrapes news articles from theblockbeats.info by going backwards through article IDs until reaching your date cutoff.\n",
    "\n",
    "## Features:\n",
    "- âœ… Scrape articles by date range (e.g., last 30 days)\n",
    "- âœ… Keyword filtering (Chinese and English)\n",
    "- âœ… Automatic stopping when reaching old articles\n",
    "- âœ… Clean body text extraction\n",
    "- âœ… CSV export with timestamp\n",
    "- âœ… Progress tracking\n",
    "\n",
    "## How it works:\n",
    "1. Starts from a recent article ID (e.g., 320007)\n",
    "2. Goes backwards through IDs (320006, 320005, ...)\n",
    "3. Stops when it finds 20 consecutive articles older than your date range\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests beautifulsoup4 lxml -q\n",
    "print(\"âœ… Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "import csv\n",
    "import time\n",
    "import re\n",
    "from google.colab import files\n",
    "\n",
    "print(\"âœ… Libraries imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Data Models and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Article:\n",
    "    url: str\n",
    "    title: str\n",
    "    publication_date: Optional[datetime]\n",
    "    author: Optional[str]\n",
    "    body_text: str\n",
    "    scraped_at: datetime\n",
    "    source_website: str\n",
    "    matched_keywords: List[str] = None\n",
    "\n",
    "print(\"âœ… Data models defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_article(url: str, session: requests.Session) -> str:\n",
    "    \"\"\"Fetch article HTML.\"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    }\n",
    "    response = session.get(url, headers=headers, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    return response.text\n",
    "\n",
    "\n",
    "def extract_date_from_body(body_text: str) -> Optional[datetime]:\n",
    "    \"\"\"Extract date from body text.\"\"\"\n",
    "    pattern = r'BlockBeats\\s*æ¶ˆæ¯\\s*ï¼Œ\\s*(\\d{1,2})\\s*æœˆ\\s*(\\d{1,2})\\s*æ—¥'\n",
    "    match = re.search(pattern, body_text)\n",
    "    if match:\n",
    "        month = int(match.group(1))\n",
    "        day = int(match.group(2))\n",
    "        year = datetime.now().year\n",
    "        try:\n",
    "            return datetime(year, month, day)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "print(\"âœ… Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define Parser Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_article(html: str, url: str) -> Article:\n",
    "    \"\"\"Parse article from HTML.\"\"\"\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "    # Extract title\n",
    "    title_tag = soup.find('h1')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"No Title\"\n",
    "    \n",
    "    # Extract body text\n",
    "    body_element = soup.select_one('.flash-top, .flash-top-border')\n",
    "    if body_element:\n",
    "        for unwanted in body_element.select('script, style, nav, header, footer'):\n",
    "            unwanted.decompose()\n",
    "        body_text = body_element.get_text(separator=' ', strip=True)\n",
    "    else:\n",
    "        body_text = \"\"\n",
    "    \n",
    "    if not body_text or len(body_text) < 50:\n",
    "        raise ValueError(\"Could not extract article body\")\n",
    "    \n",
    "    # Extract content starting from 'BlockBeats æ¶ˆæ¯' and remove footer\n",
    "    blockbeats_marker = 'BlockBeats æ¶ˆæ¯'\n",
    "    blockbeats_pos = body_text.find(blockbeats_marker)\n",
    "    \n",
    "    if blockbeats_pos != -1:\n",
    "        body_text = body_text[blockbeats_pos:]\n",
    "        \n",
    "        # Remove footer content\n",
    "        footer_markers = ['AI è§£è¯»', 'å±•å¼€', 'åŽŸæ–‡é“¾æŽ¥', 'ä¸¾æŠ¥', 'çº é”™/ä¸¾æŠ¥', 'æœ¬å¹³å°çŽ°å·²å…¨é¢é›†æˆ', 'çƒ­é—¨æ–‡ç« ']\n",
    "        earliest_pos = len(body_text)\n",
    "        for marker in footer_markers:\n",
    "            pos = body_text.find(marker)\n",
    "            if pos != -1 and pos < earliest_pos:\n",
    "                earliest_pos = pos\n",
    "        \n",
    "        if earliest_pos < len(body_text):\n",
    "            body_text = body_text[:earliest_pos]\n",
    "    \n",
    "    body_text = body_text.strip()\n",
    "    publication_date = extract_date_from_body(body_text)\n",
    "    \n",
    "    return Article(\n",
    "        url=url,\n",
    "        title=title,\n",
    "        publication_date=publication_date,\n",
    "        author=\"@BlockBeats\",\n",
    "        body_text=body_text,\n",
    "        scraped_at=datetime.now(),\n",
    "        source_website=\"www.theblockbeats.info\"\n",
    "    )\n",
    "\n",
    "print(\"âœ… Parser functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Define Filter and Save Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_save_article(article: Article, days_filter: Optional[int], keywords_filter: Optional[List[str]]) -> bool:\n",
    "    \"\"\"Check if article passes filters.\"\"\"\n",
    "    if days_filter is not None and article.publication_date:\n",
    "        cutoff_date = datetime.now() - timedelta(days=days_filter)\n",
    "        if article.publication_date < cutoff_date:\n",
    "            return False\n",
    "    \n",
    "    if keywords_filter:\n",
    "        article_text = f\"{article.title} {article.body_text}\".lower()\n",
    "        matched = [kw for kw in keywords_filter if kw.lower() in article_text]\n",
    "        \n",
    "        if not matched:\n",
    "            return False\n",
    "        \n",
    "        article.matched_keywords = matched\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def save_to_csv(articles: List[Article], filename: str):\n",
    "    \"\"\"Save articles to CSV file.\"\"\"\n",
    "    fieldnames = ['publication_date', 'title', 'body_text', 'url', 'matched_keywords']\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        for article in articles:\n",
    "            row = {\n",
    "                'publication_date': article.publication_date.strftime('%Y-%m-%d') if article.publication_date else '',\n",
    "                'title': article.title,\n",
    "                'body_text': article.body_text,\n",
    "                'url': article.url,\n",
    "                'matched_keywords': ', '.join(article.matched_keywords) if article.matched_keywords else ''\n",
    "            }\n",
    "            writer.writerow(row)\n",
    "\n",
    "print(\"âœ… Filter and save functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Define Main Scraping Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_by_date_range(start_id: int, days_back: int, keywords: List[str], output_file: str, max_articles: int = 1000):\n",
    "    \"\"\"\n",
    "    Scrape articles going backwards from start_id until reaching the date cutoff.\n",
    "    \n",
    "    Args:\n",
    "        start_id: Starting article ID (e.g., 320007 for latest)\n",
    "        days_back: How many days back to scrape\n",
    "        keywords: List of keywords to filter\n",
    "        output_file: Output CSV filename\n",
    "        max_articles: Maximum articles to attempt (safety limit)\n",
    "    \"\"\"\n",
    "    session = requests.Session()\n",
    "    articles = []\n",
    "    current_id = start_id\n",
    "    cutoff_date = datetime.now() - timedelta(days=days_back)\n",
    "    \n",
    "    consecutive_failures = 0\n",
    "    max_consecutive_failures = 10\n",
    "    articles_outside_range = 0\n",
    "    max_outside_range = 20  # Stop if we see 20 articles in a row outside date range\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ðŸ“° SCRAPING ARTICLES BY DATE RANGE\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Starting ID:     {start_id}\")\n",
    "    print(f\"Date cutoff:     {cutoff_date.strftime('%Y-%m-%d')} ({days_back} days back)\")\n",
    "    print(f\"Keywords:        {', '.join(keywords) if keywords else 'None'}\")\n",
    "    print(f\"Output file:     {output_file}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    for attempt in range(max_articles):\n",
    "        url = f\"https://www.theblockbeats.info/flash/{current_id}\"\n",
    "        \n",
    "        try:\n",
    "            print(f\"[{attempt + 1}] Checking ID {current_id}...\", end=\" \")\n",
    "            html = fetch_article(url, session)\n",
    "            article = parse_article(html, url)\n",
    "            \n",
    "            # Check if article is within date range\n",
    "            if article.publication_date and article.publication_date < cutoff_date:\n",
    "                articles_outside_range += 1\n",
    "                print(f\"â­ï¸  Too old ({article.publication_date.strftime('%Y-%m-%d')})\")\n",
    "                \n",
    "                if articles_outside_range >= max_outside_range:\n",
    "                    print(f\"\\nâœ‹ Reached {max_outside_range} consecutive articles outside date range. Stopping.\")\n",
    "                    break\n",
    "            else:\n",
    "                articles_outside_range = 0  # Reset counter\n",
    "                \n",
    "                # Check if article matches filters\n",
    "                if should_save_article(article, days_back, keywords):\n",
    "                    articles.append(article)\n",
    "                    keywords_str = f\" [{', '.join(article.matched_keywords)}]\" if article.matched_keywords else \"\"\n",
    "                    print(f\"âœ… {article.title[:50]}...{keywords_str}\")\n",
    "                else:\n",
    "                    print(f\"â­ï¸  Filtered out\")\n",
    "            \n",
    "            consecutive_failures = 0\n",
    "            time.sleep(2)  # Be respectful to the server\n",
    "            \n",
    "        except Exception as e:\n",
    "            consecutive_failures += 1\n",
    "            print(f\"âŒ Error: {str(e)[:50]}\")\n",
    "            \n",
    "            if consecutive_failures >= max_consecutive_failures:\n",
    "                print(f\"\\nâœ‹ Too many consecutive failures ({max_consecutive_failures}). Stopping.\")\n",
    "                break\n",
    "        \n",
    "        current_id -= 1\n",
    "    \n",
    "    # Save results\n",
    "    if articles:\n",
    "        print(f\"\\nðŸ’¾ Saving {len(articles)} articles to {output_file}...\")\n",
    "        save_to_csv(articles, output_file)\n",
    "        print(f\"âœ… Done! Scraped {len(articles)} articles.\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸  No articles matched the criteria.\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ðŸ“Š SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Articles scraped:  {len(articles)}\")\n",
    "    print(f\"IDs checked:       {start_id - current_id}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return output_file\n",
    "\n",
    "print(\"âœ… Main scraping function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Configure and Run Scraper\n",
    "\n",
    "Edit the configuration below and run the cell to start scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# CONFIGURATION - Edit these values\n",
    "# ===================================\n",
    "\n",
    "# Starting article ID (check the website for the latest article number)\n",
    "# Example: https://www.theblockbeats.info/flash/320007 -> use 320007\n",
    "START_ID = 320007\n",
    "\n",
    "# How many days back to scrape\n",
    "DAYS_BACK = 30  # Last 30 days (1 month)\n",
    "\n",
    "# Keywords to filter (leave empty list [] to get all articles)\n",
    "KEYWORDS = ['BTC', 'ETH', 'ç›‘ç®¡', 'Uniswap']  # Chinese and English keywords\n",
    "\n",
    "# Maximum articles to check (safety limit)\n",
    "MAX_ARTICLES = 1000\n",
    "\n",
    "# Output filename with timestamp\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "OUTPUT_FILE = f\"crypto_news_{timestamp}.csv\"\n",
    "\n",
    "# ===================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“° NEWS SCRAPER - DATE RANGE MODE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Starting ID:    {START_ID}\")\n",
    "print(f\"  Days back:      {DAYS_BACK}\")\n",
    "print(f\"  Keywords:       {', '.join(KEYWORDS) if KEYWORDS else 'None (all articles)'}\")\n",
    "print(f\"  Max articles:   {MAX_ARTICLES}\")\n",
    "print(f\"  Output file:    {OUTPUT_FILE}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nðŸš€ Starting scraper...\\n\")\n",
    "\n",
    "# Run the scraper\n",
    "output_file = scrape_by_date_range(START_ID, DAYS_BACK, KEYWORDS, OUTPUT_FILE, MAX_ARTICLES)\n",
    "\n",
    "# Download the file\n",
    "print(f\"\\nðŸ“¥ Downloading {output_file}...\")\n",
    "files.download(output_file)\n",
    "print(\"âœ… Download complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
